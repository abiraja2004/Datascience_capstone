---
title: "Milestone_report"
author: "preethi"
date: "22/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is for the capstone Datascience course. 

This is done to demonstrate that we have gotten used to working with the data and that you are on track to create your prediction algorithm

1. Demonstrate that you've downloaded the data and have successfully loaded it in.  
2. Create a basic report of summary statistics about the data sets.  
3. Report any interesting findings that you amassed so far.  
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.


## Downloading and Extracting the Data

We need to download and extract the 3 files that are under the URL : 
["https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)
We will be using files under the directory EN_US
There are three files under this directory  

1) en_US.blogs.txt - text from blog posts  
2) en_US.news.txt - text from news articles posted online  
3) en_US.twitter.txt - tweets on Twitter   

```{r Download}

fileUrl <-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
if (!file.exists("Coursera-SwiftKey.zip")){
  download.file(fileUrl, destfile = "Coursera-SwiftKey.zip", method="curl")
}
unzip("Coursera-SwiftKey.zip")
```
And then we load the three text files that we want. 

```{r readline}
setwd("~/R/datascience_coursera_main/Capstone/Datascience_capstone")
file1 <- file("final/en_US/en_US.blogs.txt", "rb")
blogs <- readLines(file1, encoding = "UTF-8",skipNul = TRUE)
close(file1)

file2 <- file("final/en_US/en_US.news.txt", "rb")
news <- readLines(file2, encoding = "UTF-8",skipNul = TRUE)
close(file2)

file3 <- file("final/en_US/en_US.twitter.txt", "rb")
twitter <- readLines(file3, encoding = "UTF-8",skipNul = TRUE)
close(file3)
```

Then we count the length of the lines and words in each of these files  

```{r words}
ls1 <- length(blogs)
ls2 <- length(news)
ls3 <- length(twitter)
linecount <- rbind(ls1, ls2, ls3)

library(stringi)
wordsz1 <- sum(stri_count_words(blogs))
wordsz2 <- sum(stri_count_words(news))
wordsz3 <- sum(stri_count_words(twitter))
wordcount <- rbind(wordsz1, wordsz2, wordsz3)

data.frame(source = c("blogs", "news", "twitter"),
           num.lines = c(ls1,ls2,ls3),
           num.words = c(wordsz1,wordsz2,wordsz3),
           mean.num.words = c(mean(wordsz1), mean(wordsz2), mean(wordsz3)))
```


```{r sampling}
set.seed(5000)

sample_size = 1000

sample_blog <- blogs[sample(1:length(blogs),sample_size)]
sample_news <- news[sample(1:length(news),sample_size)]
sample_twitter <- twitter[sample(1:length(twitter),sample_size)]
data.sample <- c(sample_blog,sample_news,sample_twitter)
```

First let us create a corpus by combining the three files

```{r corp}
library(tm)
corpus <- VCorpus(VectorSource(data.sample))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
unicorpus <- tm_map(corpus, removeWords, stopwords("en"))

```

The above step is neccesary to create a function the gives you the frequencies of the different words in the form of a data frame

```{r freqcount}
library(dplyr)
library(plyr)

getFreq <- function(tdm) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}

freq1 <- getFreq(removeSparseTerms(TermDocumentMatrix(unicorpus), 0.9999))
freq2 <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus), 0.9999))
```

```{r wordcloud}

library(wordcloud) 
pal2 <- brewer.pal(10,"Spectral")
word_cloud1 <- wordcloud(freq1$word, freq1$freq, max.words = 60, colors=pal2)
word_cloud1
  
library(wordcloud2)
word_cloud <- wordcloud2(freq2, color = "random-light",)
word_cloud

```

```{r histogram}
library(ggplot2)
library(gridExtra)
#plotMostFreq <- function(data, n_terms=30, fill_color='steelblue', title) {
plot_1  <- ggplot(freq1[1:29,], aes(x=word,y=freq)) + 
    geom_bar(stat='identity', fill="steelblue") +
    xlab('Term') + 
    ylab('Count') +
    ggtitle("30 most common words with stopwords") +
    coord_flip()
plot_2 <- ggplot(freq2[1:29,], aes(x=word,y=freq)) + 
    geom_bar(stat='identity', fill="steelblue") +
    xlab('Term') + 
    ylab('Count') +
    ggtitle("30 most common words without stopwords") +
    coord_flip()


grid.arrange(plot_1, plot_2, ncol=2)

```

uni, bi and tri grams are created and the first ten called out

```{r grams}
library(data.table)
library(RWeka)
library(tm)
library(ggplot2)
library(gridExtra)

biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bi_gram<- TermDocumentMatrix(corpus, control = list(tokenize = biGramTokenizer))
freqTerms2 <- findFreqTerms(bi_gram, lowfreq = 3)
termFreq2 <- rowSums(as.matrix(bi_gram[freqTerms2,]))
termFreq2 <- data.frame(bigram=names(termFreq2), frequency=termFreq2)
termFreq2 <- termFreq2[order(-termFreq2$frequency),]
bigramlist <- setDT(termFreq2)
save(bigramlist,file="bigram.Rds",envir = .GlobalEnv)

plot3 <- ggplot(bigramlist[1:10,], aes(x=bigram,y=frequency)) + 
    geom_bar(stat='identity', fill="steelblue") +
    xlab('Term') + 
    ylab('Count') +
    ggtitle("10 most common bigrams") +
    coord_flip()

triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tri_gram <- TermDocumentMatrix(unicorpus, control = list(tokenize = triGramTokenizer))
freqTerms3 <- findFreqTerms(tri_gram, lowfreq = 2)
termFreq3 <- rowSums(as.matrix(tri_gram[freqTerms3,]))
termFreq3 <- data.frame(trigram=names(termFreq3), frequency=termFreq3)
trigramlist <- setDT(termFreq3)
save(trigramlist,file="trigram.Rds",envir = .GlobalEnv)

plot4 <- ggplot(trigramlist[1:10,], aes(x=trigram,y=frequency)) + 
    geom_bar(stat='identity', fill="steelblue") +
    xlab('Term') + 
    ylab('Count') +
    ggtitle("10 most common trigrams") +
    coord_flip()

fourGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
four_gram <- TermDocumentMatrix(unicorpus, control = list(tokenize = fourGramTokenizer))
freqTerms4 <- findFreqTerms(four_gram, lowfreq = 1)
termFreq4 <- rowSums(as.matrix(four_gram[freqTerms4,]))
termFreq4 <- data.frame(fourgram=names(termFreq4), frequency=termFreq4)
fourgramlist <- setDT(termFreq4)
save(fourgramlist,file="fourgram.Rds",envir = .GlobalEnv)
plot5 <- ggplot(fourgramlist[1:10,], aes(x=fourgram,y=frequency)) + 
    geom_bar(stat='identity', fill="steelblue") +
    xlab('Term') + 
    ylab('Count') +
    ggtitle("10 most quadruplegrans") +
    coord_flip()

grid.arrange(plot3, plot4, plot5, nrow=1)
```

This takes too long so we will an ngrams creater which only take a sample size of 500 char
